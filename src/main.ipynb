{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","provenance":[{"file_id":"1f9lUQjTA1kQvYpgtRzo8xgh21_il1t_y","timestamp":1607619487792},{"file_id":"1pndQRuD63EzByPd9gE_8asB7zaIJg2o_","timestamp":1606190156060},{"file_id":"1IzPbxNFQmmSYk9s14L4YjBfUgACn9mW2","timestamp":1606046489500},{"file_id":"1G7NKeneJNyRtcRxLVbbF9jYtRyuTOa-R","timestamp":1592749700622},{"file_id":"https://github.com/satyajitghana/TSAI-DeepVision-EVA4.0/blob/master/Utils/Colab_25GBRAM_GPU.ipynb","timestamp":1592043804148}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"xxtDNlVJZfUX"},"source":["import os\n","import numpy as np\n","from google.colab import drive\n","from scipy.io import loadmat\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n","from sklearn.model_selection import cross_val_score\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from sklearn.preprocessing import OneHotEncoder"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_wBOHVGdmX1X","executionInfo":{"status":"ok","timestamp":1607620891685,"user_tz":-360,"elapsed":3341,"user":{"displayName":"Marat Bekmyrza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib-0tD7WjH3aGtX_5Rj6vIsFhVEXlrGvZoOgMH=s64","userId":"16583475954751235795"}},"outputId":"9a911545-8a9a-41b5-8635-07e0adc27203"},"source":["drive.mount('/content/drive')\n","os.chdir('/content/drive/MyDrive/BCI')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jhmKMEAPmrFK","executionInfo":{"status":"ok","timestamp":1607620893729,"user_tz":-360,"elapsed":5371,"user":{"displayName":"Marat Bekmyrza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib-0tD7WjH3aGtX_5Rj6vIsFhVEXlrGvZoOgMH=s64","userId":"16583475954751235795"}},"outputId":"ee7ac71f-f550-4fab-9144-9d1edc4172dd"},"source":["!pip install mne"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: mne in /usr/local/lib/python3.6/dist-packages (0.21.2)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from mne) (1.18.5)\n","Requirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from mne) (1.4.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tilDPxCGmbH1"},"source":["from src.erp import get_erp_features\n","from src.mi import MI\n","from src.ssvep import get_ssvep_features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mDvKvx-7mmNk"},"source":["def load_data(ses, sub, par):\n","    # example: 'data/session1/s1/sess01_subj01_EEG_ERP.mat'\n","    if len(str(sub))>1:\n","        filepath = f'data/session{ses}/s{sub}/sess0{ses}_subj{sub}_EEG_{par}.mat'\n","    else:  \n","        filepath = f'data/session{ses}/s{sub}/sess0{ses}_subj0{sub}_EEG_{par}.mat'\n","\n","    train_data = loadmat(filepath)[f'EEG_{par}_train'][0][0]\n","    test_data = loadmat(filepath)[f'EEG_{par}_test'][0][0]\n","    return train_data, test_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3AE73uHnmvdf"},"source":["def select_trials(data_t, data_y, num_trials=100):\n","    sampled_indices = np.random.choice(len(data_t[0]), num_trials, replace=False)\n","    return data_t[0][sampled_indices], data_y[0][sampled_indices]\n","\n","def get_data_xyt(train_data, test_data):\n","    train_t, train_y = select_trials(train_data['t'], train_data['y_dec'], 100)\n","    test_t, test_y = select_trials(test_data['t'], test_data['y_dec'], 100)\n","    test_t += train_data['x'].shape[0]  # due to concatenation\n","\n","    data_x = np.concatenate((train_data['x'], test_data['x']))\n","    data_y = np.concatenate((train_y, test_y))\n","    data_t = np.concatenate((train_t, test_t))\n","    return data_x, data_y, data_t"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GvZKpDWGmw3M"},"source":["class Features():\n","    def __init__(self):\n","        self.mi = None\n","\n","    def get_features(self, data_x, data_t, data_y=None):\n","        if data_y is not None:    \n","            assert(len(data_y)==len(data_t))\n","            self.mi = MI()\n","            mi_features = self.mi.get_mi_features(data_x, data_t, data_y)\n","        else:\n","            if self.mi is None:\n","                raise Exception(\"Give MI data first to train CSP\")\n","            else:\n","                mi_features = self.mi.get_mi_features(data_x, data_t)\n","        erp_features = get_erp_features(data_x, data_t)\n","        ssvep_features = get_ssvep_features(data_x, data_t)\n","        features = np.concatenate((mi_features, erp_features, ssvep_features), axis=1)\n","        return features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u7Uy_VgLmyJi"},"source":["def classify_LDA(train_x, train_y, test_x, test_y):\n","    clf = LDA()\n","    clf.fit(train_x, train_y)\n","    acc = clf.score(test_x, test_y)\n","    return acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CwS00QIJmzM9"},"source":["def classify_DNN(train_x, train_y, test_x, test_y):\n","    onehot_encoder = OneHotEncoder(sparse=False)\n","    train_y_hot = onehot_encoder.fit_transform(train_y.reshape(len(train_y), 1))\n","    test_y_hot = onehot_encoder.fit_transform(test_y.reshape(len(test_y), 1))\n","\n","    model = tf.keras.models.Sequential()\n","    input_size = train_x.shape[1]\n","    output_size = train_y_hot.shape[1]\n","    model.add(Dense(input_size, activation='tanh'))\n","    model.add(Dense(50, activation='tanh'))\n","    model.add(Dense(output_size, activation='softmax'))\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","    history = model.fit(train_x, train_y_hot, batch_size=8, epochs=50, verbose=0)\n","    scores = model.evaluate(test_x, test_y_hot, verbose=0)\n","    loss, accuracy = scores[0], scores[1]\n","    return accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wffYFaUGm0Wk"},"source":["def cross_val(mi_data, erp_data, ssvep_data, kfolds=5):\n","    # ERP and SSVEP don't need any correction due to CV, so they are already samples with features\n","    data_mi_x, data_mi_y, data_mi_t = mi_data                   # raw MI data, since we need to train CSP for each fold in CV\n","    data_erp_x, data_erp_y, data_erp_t = erp_data               # raw ERP data\n","    data_ssvep_x, data_ssvep_y, data_ssvep_t = ssvep_data       # raw SSVEP data\n","\n","    assert(len(data_mi_t)==len(data_erp_t))\n","    assert(len(data_mi_t)==len(data_ssvep_t))\n","\n","    fold_len = int(len(data_mi_t)/kfolds)                       # 200/5 = 40\n","    indices = np.split(np.random.choice(len(data_mi_t), size=len(data_mi_t), replace=False), kfolds)\n","\n","    lda_cv_scores, dnn_cv_scores = [], []\n","    for k in range(0, kfolds):\n","        # data division part\n","        #####\n","        inds = indices.copy()\n","        test_inds = np.array(inds.pop(k)).flatten()\n","        train_inds = np.array(inds).flatten()\n","        \n","        ft = Features()\n","\n","        mi_train_y, mi_train_t = data_mi_y[train_inds], data_mi_t[train_inds]\n","        mi_test_y, mi_test_t = data_mi_y[test_inds], data_mi_t[test_inds]\n","        mi_train_x = ft.get_features(data_mi_x, mi_train_t, mi_train_y)   # here we get features by training CSP\n","        mi_test_x = ft.get_features(data_mi_x, mi_test_t)                  # here we get features using trained CSP\n","        \n","        erp_x = ft.get_features(data_erp_x, data_erp_t)\n","        erp_train_x, erp_train_y = erp_x[train_inds], data_erp_y[train_inds]\n","        erp_test_x, erp_test_y = erp_x[test_inds], data_erp_y[test_inds]\n","\n","        ssvep_x = ft.get_features(data_ssvep_x, data_ssvep_t)\n","        ssvep_train_x, ssvep_train_y = ssvep_x[train_inds], data_ssvep_y[train_inds]\n","        ssvep_test_x, ssvep_test_y = ssvep_x[test_inds], data_ssvep_y[test_inds]\n","        \n","        ##\n","        train_x = np.concatenate((erp_train_x, ssvep_train_x, mi_train_x))              # concatenated features\n","        train_y = np.concatenate((erp_train_y, ssvep_train_y + 4, mi_train_y + 2))      # concatenated labels\n","\n","        test_x = np.concatenate((erp_test_x, ssvep_test_x, mi_test_x))                  # concatenated features\n","        test_y = np.concatenate((erp_test_y, ssvep_test_y + 4, mi_test_y + 2))          # concatenated labels\n","        #####\n","        \n","        # classifier part\n","        # can add another classifiers here\n","        #####\n","        lda_accuracy = classify_LDA(train_x, train_y, test_x, test_y)\n","        dnn_accuracy = classify_DNN(train_x, train_y, test_x, test_y)\n","        lda_cv_scores.append(lda_accuracy)\n","        dnn_cv_scores.append(dnn_accuracy)\n","        #####\n","\n","    lda_average = np.average(lda_cv_scores)\n","    dnn_average = np.average(dnn_cv_scores)\n","\n","    return lda_average, dnn_average"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BcigltyEm1pA"},"source":["def main_loop(sessions, subjects):\n","    accuracies = {}\n","    for ses in sessions:\n","        accuracies[ses] = {}\n","        for sub in subjects:\n","            accuracies[ses][sub] = {}\n","            # MI\n","            train_mi_data, test_mi_data = load_data(ses, sub, 'MI')\n","            mi_data = get_data_xyt(train_mi_data, test_mi_data)\n","            \n","            # ERP\n","            train_erp_data, test_erp_data = load_data(ses, sub, 'ERP')\n","            erp_data = get_data_xyt(train_erp_data, test_erp_data)\n","            \n","            # SSVEP\n","            train_ssvep_data, test_ssvep_data = load_data(ses, sub, 'SSVEP')\n","            ssvep_data = get_data_xyt(train_ssvep_data, test_ssvep_data)\n","\n","            # Cross Validation\n","            lda, dnn = cross_val(mi_data, erp_data, ssvep_data, kfolds=5)\n","            accuracies[ses][sub]['LDA average'] = lda\n","            accuracies[ses][sub]['DNN average'] = dnn\n","            print(f'Session {ses}, Subject {sub}, LDA {lda}, DNN {dnn}')\n","    return accuracies"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AqNjjLrSm3Fc"},"source":["sessions = ['1', '2']\n","subjects = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ncne1Z36m4KK","outputId":"64bed163-fc13-4675-c9e3-4a061c932e30"},"source":["accs = main_loop(sessions, subjects)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Session 1, Subject 1, LDA 0.635\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/cross_decomposition/_pls.py:96: ConvergenceWarning: Maximum number of iterations reached\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/cross_decomposition/_pls.py:96: ConvergenceWarning: Maximum number of iterations reached\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/cross_decomposition/_pls.py:96: ConvergenceWarning: Maximum number of iterations reached\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/cross_decomposition/_pls.py:96: ConvergenceWarning: Maximum number of iterations reached\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/cross_decomposition/_pls.py:96: ConvergenceWarning: Maximum number of iterations reached\n","  ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Session 1, Subject 2, LDA 0.7766666666666667\n","Session 1, Subject 3, LDA 0.745\n","Session 1, Subject 4, LDA 0.6950000000000001\n","Session 1, Subject 5, LDA 0.7683333333333333\n","Session 1, Subject 6, LDA 0.735\n","Session 1, Subject 7, LDA 0.76\n","Session 1, Subject 8, LDA 0.7\n","Session 1, Subject 9, LDA 0.6833333333333333\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hYsjyRpSqZ3T"},"source":[""],"execution_count":null,"outputs":[]}]}